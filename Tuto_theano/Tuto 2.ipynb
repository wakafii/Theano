{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differents éléments d'un réseau de neurone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu les differentes fonctionnalités de base de theano précédemment, nous allons maintenant pouvoir nous interesser au différents blocs qui peuvent former un réseau de neuronne. Pour commencer nous avons vu un exemple de modèle de réseau de neuronne au tutoriel précédent. En général un réseau de neuronne sera composé d'un certain nombre de neuronne, ces neuronnes seront repartis en \"layer\" et ainsi chacun d'eux appliquera une opération mathématique sur ses entrées afin de générer sa sortie. Nous allons dans ce tutoriel voir les différents \"layer\" que nous pouvons utiliser dans un réseau de neuronnes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier type de \"layer\" que nous verrons est le softmax, il permet de transformer les valeurs des neurones en probabilité: $$\\frac{valeur\\_neuronne}{\\sum_{k=0}^N valeur\\_neuronne}$$  \n",
    "Le Softmax sera toujours suivi d'une autre fonction : argmax, qui retournera l'indice du neuronne avec la plus haute probabilité. L'ensemble de ces deux fonctions est souvent utilisé en tant derniere couche de neurone afin de réaliser la prédiction.  \n",
    "Afin d'utiliser la fonction softmax de theano nous avons besoin de définir des poids et des biais, pour cela nous les déclarons en tant que variables partagées.  \n",
    "Commençons par importer les librairies nécessaire à ce tuto :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant commencer à coder notre softmax :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 2\n",
    "\n",
    "# On déclare les poids\n",
    "W = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "        (0, 10),\n",
    "        dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='W',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# On déclare les biais\n",
    "b = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "        (10,),\n",
    "        dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='b',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# On appele la fonction softmax\n",
    "p_y_given_x = T.nnet.softmax(T.dot(input, W) + b)\n",
    "\n",
    "# On stocke les poids et biais\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la fonction argmax nous n'avons besoin que de la sortie de la fonction softmax :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = T.argmax(p_y_given_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les fonctions conv2d et pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant voir un puissant outil des réseau de neurones: le layer convolutionnal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il va fonctionner de la manière suivante: il applique un filtre sur une partie de la matrice d'entrée à la fois afin de créer une nouvelle matrice contenant plus d'information concentrée, le filtre sera dimensionné de sorte à récuperer les informations importantes.  \n",
    "![Image 1](img/deep-learning-with-keras-48-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser la fonction conv2d nous avons besoin de définir plusieurs variables, la première sera la matrice d'entrée qui devra être redimensionnée pour correspondre à la taille d'entrée que nous indiquons à la fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=600 # nombre de donnée en entrée\n",
    "x = T.matrix('x')\n",
    "input = x.reshape((batch_size, 1, 28, 28))\n",
    "image_shape = (batch_size, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois l'entrée définit il nous faut définir un filtre pour la convolution ainsi que la taille de ce filtre, pour cela nous utiliserons une variable partagée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nkerns = [20, 50] # définit la \"profondeur\" des filtres\n",
    "poolsize=(2, 2)\n",
    "\n",
    "filter_shape = (nkerns[0], 1, 5, 5)\n",
    "\n",
    "fan_in = numpy.prod(filter_shape[1:])\n",
    "\n",
    "fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) //\n",
    "           numpy.prod(poolsize))\n",
    "\n",
    "W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "\n",
    "W = theano.shared(\n",
    "    numpy.asarray(\n",
    "        numpy.random.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "        dtype=theano.config.floatX\n",
    "    ),\n",
    "    borrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois toutes les variables définies nous pouvons appeler la fonction conv2d, ainsi que sauvegarder ses paramètres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_out = T.nnet.conv2d(\n",
    "    input=input,\n",
    "    filters=W,\n",
    "    filter_shape=filter_shape,\n",
    "    input_shape=image_shape\n",
    ")\n",
    "\n",
    "params = [W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction maxpool va permettre de reduire la taille de la matrice sur laquelle on travaille, pour cela il nous faudra définir une poolSize qui sera la taille du kernel sur laquelle la fonction agira, la fonction prendra le max de chaque partie de la matrice afin de créer une nouvelle matrice de sortie. Exemple: avec une poolSize de (2,2) la taille de la matrice sera divisée par 2.  \n",
    "Cette fonction est utilisée essentiellement après un conv layer pour récuperer les information les plus importantes.  \n",
    "![image2](img/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano.tensor.signal import pool\n",
    "pooled_out = pool.pool_2d(\n",
    "    input=conv_out,\n",
    "    ws=poolsize,\n",
    "    ignore_border=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les fonctions pour des réseaux dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe un certain nombre de fonction pour créer des réseaux dense dans la librairie theano.tensor.nnet, elle permette de créer des réseaux classique, mais peuvent aussi être utilisées dans des réseaux plus complexe pour rajouter une étape à un réseau, cela permet parfois de rendre le réseau plus puissant. Voici une liste des fonctions que l'on peut trouver dans dans cette librairie:\n",
    "\n",
    "\n",
    "    Sigmoid\n",
    "            sigmoid()\n",
    "            ultra_fast_sigmoid()\n",
    "            hard_sigmoid()\n",
    "\n",
    "    Others\n",
    "            softplus()\n",
    "            softmax()\n",
    "            softsign()\n",
    "            relu()\n",
    "            elu()\n",
    "            selu()\n",
    "            binary_crossentropy()\n",
    "            sigmoid_binary_crossentropy()\n",
    "            categorical_crossentropy()\n",
    "            h_softmax()\n",
    "            confusion_matrix\n",
    "\n",
    "Nous avons déja vu la fonction Softmax dans ce tutoriel, toutes ces fonctions vont marcher selon plus ou moins le même principe que Softmax, vous pouvez trouver une aide pour chacune de ces fonctions sur le site officiel de [theano](http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons ensemble une de ces fonctions qui est très utilisée dans les réseaux de neurones : la fonction relu.  \n",
    "Pour cette fonction nous avons besoin de déclarer des poids et des biais comme pour la Softmax, pour cela nous utiliserons donc des variables partagées. Cette fonction va mettre à 0 tous les neurones à valeur négative et laisser les autres à leur valeur.  \n",
    "![image 3](img/relu-activation-function-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 2\n",
    "\n",
    "# On déclare les poids\n",
    "W1 = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "        (0, 10),\n",
    "        dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='W',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# On déclare les biais\n",
    "b1 = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "        (10,),\n",
    "        dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='b',\n",
    "    borrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous faisons l'appel à notre fonction relu et nous enregistrons ses paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y_given_x = T.nnet.relu(T.dot(input, W1) + b1, alpha=0)\n",
    "\n",
    "# On stocke les poids et biais\n",
    "params = [W1, b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe aussi une autre fonction dont nous n'avons pas encore parlé : la fonction tanh, elle va avoir le même effet que la fonction sigmoid mais entre l'intervalle [-1;1] alors que la sigmoid agit sur l'intervalle [0;1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu quelque fonctions pour créer un réseau de neurones, il en existe beaucoup que je vous laisserai découvrir par vous même, vous avez maintenant toutes les cartes en main pour avancer. Il ne reste une derniere chose à voir : les patterns de layer convolutionnal. Il en existe un qui peut être utilisé :  \n",
    "INPUT -> [[CONV -> RELU]\\*N -> POOL?]\\*M -> [FC -> RELU]\\*K -> FC  \n",
    "  \n",
    "N,M,K sont des constantes qui indique le nombre de fois ou le layer est répété, il est possible d'avoir N,M,K = 0.  \n",
    "? indique que l'on peut ou non mettre le layer  \n",
    "CONV = layer convolutionnal  \n",
    "RELU = layer relu  \n",
    "POOL = layer maxpool  \n",
    "FC = layer fully connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le prochain tutoriel nous mettrons tout cela en pratique pour créer un réseau de neurone répondant au problème de la détection de chiffre : le MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
